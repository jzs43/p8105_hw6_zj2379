---
title: "p8105_hw6_zj2379"
author: "Zheshu Jiang"
date: "2023-11-28"
output: github_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(dplyr)
library(ggplot2)
library(modelr)
```
# Problem 0

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

# Problem 1
```{r}
homicide_df = 
  read_csv("homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1),
    victim_age = as.numeric(victim_age))|>
  filter(victim_race == "Black" | victim_race == "White")|> 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```
Save the output of glm as an R object; apply the broom::tidy to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r}
baltimore_data= homicide_df |> filter(city_state=="Baltimore, MD")|>
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_data |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```
Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.
```{r q1_glm_all_cities}
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

```{r q1_plot}
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Problem 2
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```
 We’ll focus on a simple linear regression with tmax as the response with tmin and prcp as the predictors, and are interested in the distribution of two quantities estimated from these data:

```{r}
## construct a simple linear regression 
slr_weather_df <- weather_df |>
  lm(tmax ~ tmin + prcp, data = _) 
# extract the r^2
r_squared= slr_weather_df |>broom::glance()|>pull(r.squared)
# extract the log(estimated beta1 + estimated beta2)
log_product=slr_weather_df |> broom::tidy() |>
  filter(term != "(Intercept)") |>
  summarise(log_product = log(prod(estimate)))
```


```{r}
## bootstrap
boot_sample = function(df) {
   sample_frac(df, replace = TRUE)
}

boot_straps = 
  tibble(strap_number = 1:200) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

bootstrap_results = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = weather_df) ),
    results = map(models, broom::tidy),
  glance = map(models, broom::glance))|> 
  select(-strap_sample, -models) |> 
  unnest(results)

log_product=bootstrap_results|>
group_by(strap_number)|>
  filter(term != "(Intercept)")|>
  summarize(estimate_product = prod(estimate))|>
  mutate(log_estimate_product = log(estimate_product))

r_squared_values <- bootstrap_results |>select(glance)|>unnest(glance)|>select(r.squared)
```
# Problem 3
```{r}
birthweight=
  read_csv("birthweight.csv")|>
  mutate(
    across(c(babysex, frace, malform, mrace, parity), as.factor)
  )|>
  drop_na()
```
Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process and show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot.
```{r}
reg_1=lm(bwt ~ blength + bhead + smoken + wtgain + babysex + delwt + fincome + frace + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppwt +  menarche + gaweeks + smoken, data = birthweight)
summary(reg_1)
```


```{r}
birthweight <- birthweight |>
  add_predictions(reg_1, var = "fitted_values") |>
  add_residuals(reg_1, var = "residuals")

ggplot(birthweight, aes(x = fitted_values, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red") +
  labs(x = "Fitted Values", y = "Residuals") +
  theme_minimal()
```
From the plot, the residuals scatter around 0.

```{r}
reg_2=lm(bwt ~ blength+ gaweeks, data = birthweight)
summary(reg_2)

reg_3=lm(bwt ~ bhead+ blength+ babysex+ bhead * blength + bhead * babysex + blength * babysex, data = birthweight)
summary(reg_3)
```
```{r}
cv_df = 
  crossv_mc(birthweight, 100) 

cv_df |> pull(train) |> nth(1) |> as_tibble()
cv_df |> pull(test) |> nth(1) |> as_tibble()

cv_df =
  cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_results =
  cv_df |> 
  mutate(
    reg_1 = map(train, \(df) lm(bwt ~ blength + bhead + smoken + wtgain + babysex + delwt + fincome + frace + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppwt +  menarche + gaweeks + smoken, data = birthweight)),
    reg_2 = map(train, \(df) lm(bwt ~ gaweeks + blength, data = birthweight)),
    reg_3 = map(train, \(df) lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex, 
             data = birthweight))
  ) |> 
  mutate(
    rmse_reg_1 = map2_dbl(reg_1, test, \(mod, df) rmse(mod, df)),
    rmse_reg_2= map2_dbl(reg_2, test, \(mod, df) rmse(mod, df)),
    rmse_reg_3 = map2_dbl(reg_3, test, \(mod, df) rmse(mod, df))
  )
```

```{r}
cv_results |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model_type",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  group_by(model_type) |> 
  summarize(m_rmse = mean(rmse))
```

```{r}
cv_results  |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```
```{r}
cv_results |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model_type",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  ggplot(aes(x = model_type, y = rmse)) +
  geom_violin()
```

Reg_1 might have the best performance in terms of RMSE because its distribution is lower and tighter, indicating lower and more consistent errors.